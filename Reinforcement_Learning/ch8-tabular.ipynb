{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Environment Tests\n",
    "Gridworld where a shortcut opens up halfway through. \n",
    "\n",
    "With agents that learn __and__ plan\n",
    "\n",
    "\n",
    "* Position in the gridworld is the state\n",
    "* Actions are compass moves between positions.\n",
    "* Reward is 0 everywhere, except when reaching the goal, where it is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    '''Class defining the environment'''\n",
    "    def __init__(self, shapex, shapey, walls, start, goal):\n",
    "        self.shapex = shapex\n",
    "        self.shapey = shapey\n",
    "        self.walls  = walls\n",
    "        self.start  = start\n",
    "        self.goal   = goal\n",
    "        self.idx    = self.build_index()\n",
    "        self.size   = shapex * shapey\n",
    "        \n",
    "        self.set_bitmap()\n",
    "        self.original_walls = walls.copy()\n",
    "        \n",
    "    def reset_walls(self):\n",
    "        '''In a dynamic environment reset the walls to the original state'''\n",
    "        self.walls  = self.original_walls\n",
    "        self.set_bitmap()\n",
    "        \n",
    "    def state_from_index(self, number):\n",
    "        '''get state from index'''\n",
    "        x = number // self.shapey\n",
    "        y = number  % self.shapey\n",
    "        \n",
    "        return (x, y)\n",
    "        \n",
    "    def build_index(self):\n",
    "        '''build index for indexing states'''\n",
    "        return {(x, y) : self.shapey * x + y for x in range(self.shapex) \n",
    "                for y in range(self.shapey)}\n",
    "        \n",
    "    def set_bitmap(self):\n",
    "        '''builds the bitmap for visualisation'''\n",
    "        bitmap = np.ones((self.shapex, self.shapey))\n",
    "        \n",
    "        for wall in self.walls:\n",
    "            bitmap[wall] = 0\n",
    "        \n",
    "        self.bitmap = bitmap\n",
    "    \n",
    "    def show_bitmap(self):\n",
    "        '''shows the environment of holes and walkable path'''\n",
    "        plt.imshow(self.bitmap)\n",
    "        \n",
    "    def change(self, add=[], remove=[]):\n",
    "        '''add or remove walls'''\n",
    "        self.walls.extend([i for i in add])\n",
    "        self.walls = [wall for wall in self.walls if wall not in remove]\n",
    "        self.set_bitmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shapex = 6\n",
    "shapey = 9\n",
    "start  = (5, 5)\n",
    "goal   = (0, 8)\n",
    "walls  = [(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8)]\n",
    "\n",
    "shortcut = Maze(shapex, shapey, walls, start, goal)\n",
    "shortcut.show_bitmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "### Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95):\n",
    "        self.maze       = maze\n",
    "        self.moves      = moves\n",
    "        self.plan_steps = plan_steps\n",
    "        self.alpha      = alpha\n",
    "        self.epsilon    = epsilon\n",
    "        self.gamma      = gamma\n",
    "        self.move_idx   = {moves[i] : i for i in range(len(moves))}\n",
    "    \n",
    "    # -------------------------------------------Methods-------------------------------------------------#\n",
    "    \n",
    "    def reset(self):\n",
    "        '''Called once in the beginning to initialize variables'''\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.history = [set()] * self.maze.size # History for planning part\n",
    "        self.history_full = np.zeros(self.maze.size, dtype=bool)\n",
    "        \n",
    "            \n",
    "    def step(self, S, A):\n",
    "        '''Move and record to history'''\n",
    "        Sn, R = self.move(S, A)\n",
    "\n",
    "        # Only add to History if it isnt full yet\n",
    "        if not self.history_full[self.maze.idx[S]]:\n",
    "            self.history[self.maze.idx[S]].add(A)\n",
    "            if len(self.history[self.maze.idx[S]]) == 4:\n",
    "                self.history_full[self.maze.idx[S]] = 1\n",
    "                \n",
    "        self.visited = [i for i, h in enumerate(self.history) if h]\n",
    "        return Sn, R\n",
    "        \n",
    "    def q_update(self, S, A, R, Sn):\n",
    "        '''Update State-Action-Values'''\n",
    "        Am       = np.argmax(self.Q[Sn])\n",
    "        idx      = (S[0],  S[1],  A)\n",
    "        idxn     = (Sn[0], Sn[1], Am)\n",
    "        self.Q[idx] += self.alpha * (R + self.gamma * self.Q[idxn] - self.Q[idx])\n",
    "        \n",
    "    def m_update(self, S, A, R, Sn):\n",
    "        '''Update Model'''\n",
    "        idx         = (S[0], S[1], A)\n",
    "        self.M[idx] = R, Sn[0], Sn[1]\n",
    "        \n",
    "    def update(self, S, A, R, Sn):\n",
    "        '''To easily call from training function'''\n",
    "        self.q_update(S, A, R, Sn)\n",
    "        self.m_update(S, A, R, Sn)\n",
    "        \n",
    "    def plan(self):\n",
    "        '''Planning section'''\n",
    "        for step in range(self.plan_steps):\n",
    "            Spi = random.choice(self.visited)\n",
    "            Sp  = self.maze.state_from_index(Spi)\n",
    "            Ap  = random.choice(tuple(self.history[Spi]))\n",
    "            Rp  = self.M[Sp[0], Sp[1], Ap, 0]\n",
    "            Spn = tuple(self.M[Sp[0], Sp[1], Ap, 1:])\n",
    "            \n",
    "            self.q_update(Sp, Ap, Rp, Spn)\n",
    "            \n",
    "    def done(self, S):\n",
    "        '''Have we reached the goal?'''\n",
    "        return S == self.maze.goal\n",
    "    \n",
    "    # --------------------------------------------Setup--------------------------------------------------#\n",
    "    \n",
    "    def move(self, S, A):\n",
    "        '''Move from state S with action A giving new state Sn and reward R'''\n",
    "        # Clamp values to map\n",
    "        direction = self.moves[A]\n",
    "        \n",
    "        Sx  = int(max(0, min(self.maze.shapex - 1, S[0] + direction[0])))\n",
    "        Sy  = int(max(0, min(self.maze.shapey - 1, S[1] + direction[1])))\n",
    "        Sn = (Sx, Sy)\n",
    "\n",
    "        # Reset position if falling in hole\n",
    "        if Sn in self.maze.walls:\n",
    "            Sn = S\n",
    "\n",
    "        R = 1 if Sn == self.maze.goal else 0\n",
    "\n",
    "        return Sn, R\n",
    "    \n",
    "    def greedy(self, S):\n",
    "        '''Greedy policy'''\n",
    "        return np.argmax(self.Q[S])\n",
    "    \n",
    "    def epsilon_greedy(self, S):\n",
    "        '''Epsilon greedy policy'''\n",
    "        return np.argmax(self.Q[S]) if np.random.rand() > self.epsilon else np.random.randint(len(moves))\n",
    "    \n",
    "    def trace(self):\n",
    "        '''Path following greedy policy'''\n",
    "        S = self.maze.start\n",
    "        path = []\n",
    "\n",
    "        while not self.done(S):\n",
    "            path.append(S)\n",
    "            A    = self.greedy(S)\n",
    "            S, _ = self.step(S, A)\n",
    "\n",
    "            # Loop detection\n",
    "            if S in path:\n",
    "                print(\"State Values dont create greedy path\")\n",
    "                break\n",
    "\n",
    "        return path\n",
    "    \n",
    "    # -------------------------------------------Imaging-------------------------------------------------#\n",
    "    \n",
    "    def create_best_path_img(self):\n",
    "        '''Create grid in which best path is highlighted'''\n",
    "        path = self.trace()\n",
    "        \n",
    "        c0 = 5\n",
    "        c1 = 6\n",
    "        \n",
    "        length    = len(path)\n",
    "        grid      = self.maze.bitmap.copy()\n",
    "        fade      = np.linspace(c0, c1, num=length)\n",
    "        \n",
    "        for i in range(length):\n",
    "            grid[path[i]] = fade[i]\n",
    "            \n",
    "        grid[self.maze.start] = c0\n",
    "        grid[self.maze.goal]  = c1\n",
    "        \n",
    "        print(f\"path length: {length}\")\n",
    "        return grid\n",
    "    \n",
    "    def create_direction_map_img(self):\n",
    "        '''Create grid with best path and best action for every state'''\n",
    "        arr_len    = 0.85\n",
    "        grid       = self.create_best_path_img()\n",
    "        directions = [(S, self.greedy(S))\n",
    "                      for x in range(self.maze.shapex) \n",
    "                      for y in range(self.maze.shapey) if (S := (x, y))]   \n",
    "\n",
    "        for i in directions:\n",
    "            if i[0] not in self.maze.walls and i[0] != self.maze.goal:\n",
    "                    dx = arr_len if i[1] == 2 else -arr_len if i[1] == 1 else 0 \n",
    "                    dy = arr_len if i[1] == 3 else -arr_len if i[1] == 0 else 0\n",
    "                    plt.arrow(i[0][1], i[0][0], dx, dy, head_width=0.1)\n",
    "\n",
    "        return grid\n",
    "    \n",
    "    def draw_path_and_dir_map(self):\n",
    "        '''Plot best path and best action for every graph'''\n",
    "        plt.imshow(self.create_direction_map_img())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+\n",
    "Additional variable $\\tau$ tracking for each state how many timesteps have gone by since it was last visited.\n",
    "\n",
    "In the planning phase we add $k \\cdot \\sqrt{\\tau}$ to the planned reward. This changes the estimated values for states and actions but _encourages_ exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlus(DynaQ):\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=0.00001):\n",
    "        super().__init__(maze, moves, plan_steps, alpha, epsilon, gamma)\n",
    "        self.k = k\n",
    "        \n",
    "    def reset(self):\n",
    "        '''Called once in the beginning to initialize variables'''\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.T = np.zeros(self.maze.size)\n",
    "        self.history      = [set()] * self.maze.size # History for planning part\n",
    "        self.history_full = np.zeros(self.maze.size)\n",
    "            \n",
    "    def step(self, S, A):\n",
    "        '''Move and record to history\n",
    "        Also update time of last visit to states'''\n",
    "        Sn, R = self.move(S, A)\n",
    "        self.T[self.maze.idx[S]] = 0\n",
    "\n",
    "        # Only add to History if it isnt full yet\n",
    "        if not self.history_full[self.maze.idx[S]]:\n",
    "            self.history[self.maze.idx[S]].add(A)\n",
    "            if len(self.history[self.maze.idx[S]]) == 4:\n",
    "                self.history_full[self.maze.idx[S]] = 1\n",
    "                \n",
    "        self.visited = [i for i, h in enumerate(self.history) if h]\n",
    "        \n",
    "        for i in self.visited:\n",
    "            self.T[i] += 1\n",
    "            \n",
    "        return Sn, R\n",
    "                \n",
    "    def plan(self):\n",
    "        '''Planning part including bonus reward for older states'''\n",
    "        for step in range(self.plan_steps):\n",
    "            Spi = random.choice(self.visited)\n",
    "            Sp  = self.maze.state_from_index(Spi)\n",
    "            Ap  = random.choice(tuple(self.history[Spi]))\n",
    "            Rp  = self.M[Sp[0], Sp[1], Ap, 0] + self.k * np.sqrt(self.T[Spi])\n",
    "            Spn = tuple(self.M[Sp[0], Sp[1], Ap, 1:])\n",
    "            \n",
    "            self.q_update(Sp, Ap, Rp, Spn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+Experiment (Ex. 8.4)\n",
    "Instead of adding $k \\cdot \\sqrt{\\tau}$ to the planned reward, we change our greedy action to be greedy towards the reward $R + k \\cdot \\sqrt{\\tau}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlusExperiment(DynaQ):\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=0.00001):\n",
    "        super().__init__(maze, moves, plan_steps, alpha, epsilon, gamma)\n",
    "        self.k = k\n",
    "        \n",
    "    def reset(self):\n",
    "        '''Called once in the beginning to initialize variables'''\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.T = np.zeros(self.maze.size)\n",
    "        self.history      = [set()] * self.maze.size # History for planning part\n",
    "        self.history_full = np.zeros(self.maze.size)\n",
    "            \n",
    "    def step(self, S, A):\n",
    "        '''Move and record to history\n",
    "        Also update time of last visit to states'''\n",
    "        Sn, R = self.move(S, A)\n",
    "        self.T[self.maze.idx[S]] = 0\n",
    "\n",
    "        # Only add to History if it isnt full yet\n",
    "        if not self.history_full[self.maze.idx[S]]:\n",
    "            self.history[self.maze.idx[S]].add(A)\n",
    "            if len(self.history[self.maze.idx[S]]) == 4:\n",
    "                self.history_full[self.maze.idx[S]] = 1\n",
    "                \n",
    "        self.visited = [i for i, h in enumerate(self.history) if h]\n",
    "        \n",
    "        for i in self.visited:\n",
    "            self.T[i] += 1\n",
    "        \n",
    "        return Sn, R\n",
    "    \n",
    "    def epsilon_greedy(self, S):    \n",
    "        '''Epsilon greedy policy including bonus to older states'''\n",
    "        i = np.argmax([self.Q[Sn][i] + self.k * np.sqrt(self.T[self.maze.idx[Sn]]) \n",
    "                       for i in range(len(self.moves)) \n",
    "                       if (Sn := self.move(S, i)[0])])\n",
    "        \n",
    "        return i if np.random.rand() > self.epsilon else np.random.randint(len(moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Sweeping\n",
    "In DynaQ we plan updates from a random sampling of states, in Prioritized Sweeping we have a queue which prioritizes updates by size of the update. Parameter $\\theta$ determines the minimum size of an update.\n",
    "\n",
    "The updates then start at the biggest change and work their way to the smallest change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedSweep(DynaQ):\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, theta=0.1):\n",
    "        super().__init__(maze, moves, plan_steps, alpha, epsilon, gamma)\n",
    "        self.theta = theta\n",
    "    \n",
    "    def reset(self):\n",
    "        '''Called once in the beginning to initialize variables'''\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.PQueue = []\n",
    "        \n",
    "    def p_update(self, S, A, R, Sn):\n",
    "        '''Insert state-action pair into queue if P is big enough'''\n",
    "        Am       = np.argmax(self.Q[Sn])\n",
    "        idx      = (S[0],  S[1],  A)\n",
    "        idxn     = (Sn[0], Sn[1], Am)\n",
    "        P        = abs(R + self.gamma * self.Q[idxn] - self.Q[idx])\n",
    "        if P > self.theta:\n",
    "            self.try_to_insert(P, S, A)\n",
    "            \n",
    "    def step(self, S, A):\n",
    "        '''Move to next state'''\n",
    "        Sn, R = self.move(S, A)\n",
    "        return Sn, R\n",
    "    \n",
    "    def update(self, S, A, R, Sn):\n",
    "        '''Update function to be used in the train function'''\n",
    "        self.m_update(S, A, R, Sn)\n",
    "        self.p_update(S, A, R, Sn)\n",
    "        \n",
    "    def plan(self):\n",
    "        '''Rather complex planning part.\n",
    "        Working through the queue, \n",
    "        updating state-action values,\n",
    "        adding more state-action pairs to the queue'''\n",
    "        for step in range(self.plan_steps):\n",
    "            if len(self.PQueue) > 0:\n",
    "                _, S, A = self.PQueue.pop(0)\n",
    "                idx     = (S[0],  S[1],  A)\n",
    "                R       = self.M[idx][0]\n",
    "                Sn      = tuple(self.M[idx][1:])\n",
    "                self.q_update(S, A, R, Sn)\n",
    "                \n",
    "                # Catch all states leading to S according to Model M\n",
    "                MP = []\n",
    "                for x in range(self.maze.shapex):\n",
    "                    for y in range(self.maze.shapey):\n",
    "                        for a in range(len(self.moves)):\n",
    "                            if S == tuple(self.M[x, y, a][1:]):\n",
    "                                D = self.M[x, y, a]\n",
    "                                MP.append((D[0], D[1], D[2], a))\n",
    "                \n",
    "                for i in range(len(MP)):\n",
    "                    Rp, Spx, Spy, Ap = MP[i]\n",
    "                    self.p_update((Spx, Spy), Ap, Rp, S)\n",
    "    \n",
    "    def try_to_insert(self, P, S, A):\n",
    "        '''Find out if S, A already exists in queue\n",
    "        Only insert if new P value is bigger'''\n",
    "        to_insert = (P, S, A)\n",
    "        found     = False\n",
    "        idx       = 0\n",
    "        \n",
    "        for idx in range(len(self.PQueue)):\n",
    "            if self.PQueue[idx][1] == S and self.PQueue[idx][2] == A:\n",
    "                found = True\n",
    "                if P > self.PQueue[idx][0]:\n",
    "                    insert_idx = self.insert_into_queue(to_insert, idx + 1)\n",
    "                    self.remove_lower_values(insert_idx, S, A)\n",
    "                    break\n",
    "                    \n",
    "        if not found:\n",
    "            self.PQueue.append(to_insert)\n",
    "            \n",
    "    def insert_into_queue(self, tup, max_idx):\n",
    "        '''Insert P, S, A tuple into queue, its maximum index can be max_idx'''\n",
    "        for idx in range(max_idx):\n",
    "            if tup[0] > self.PQueue[idx][0]:\n",
    "                self.PQueue.insert(idx, tup)\n",
    "                break\n",
    "            \n",
    "        return idx\n",
    "    \n",
    "    def remove_lower_values(self, idx, S, A):\n",
    "        '''Removes lower values of same state-action.\n",
    "        Can break after finding one because \n",
    "        we only keep one of it in the queue'''\n",
    "        while idx < len(self.PQueue):\n",
    "            if self.PQueue[idx][1] == S and self.PQueue[idx][2] == A:\n",
    "                del self.PQueue[idx]\n",
    "                break\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Shortcut world, shortcut opens when we're halfway through the learning process.\n",
    "If the agent doesn't reach the goal after 50 episodes I cut it off without reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 6000\n",
    "moves    = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "world    = shortcut\n",
    "switch   = 3000\n",
    "remove   = [(3, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, world, switch=-1, add=[], remove=[]):\n",
    "    agent.reset()    \n",
    "    world.reset_walls()\n",
    "    reward_history = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        S = world.start\n",
    "        t = 0\n",
    "\n",
    "        if episode == switch:\n",
    "            world.change(add=add, remove=remove)\n",
    "\n",
    "        while True:\n",
    "            A     = agent.epsilon_greedy(S)\n",
    "            Sn, R = agent.step(S, A)\n",
    "            agent.update(S, A, R, Sn)\n",
    "\n",
    "            S = Sn\n",
    "            agent.plan()\n",
    "            \n",
    "            t += 1\n",
    "            if agent.done(S) or t > 50:\n",
    "                reward_history.append((R, t))\n",
    "                break\n",
    "\n",
    "    return np.array(reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent   = DynaQ(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95)\n",
    "reward1 = train(agent, world, switch=switch, remove=remove)\n",
    "agent.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+ Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2  = DynaQPlus(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=10e-3)\n",
    "reward2 = train(agent2, world, switch=switch, remove=remove)\n",
    "agent2.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+Experiment Run\n",
    "(takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent3  = DynaQPlusExperiment(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=0.001)\n",
    "reward3 = train(agent3, world, switch=switch, remove=remove)\n",
    "agent3.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorized Sweeping Run\n",
    "The prioritization of updates leads less timesteps to converge to the optimal path. \n",
    "But we run 6k episodes just like all of the other methods, so we only see the speedup in the graph later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path length: 18\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAD4CAYAAAA94VfoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkbUlEQVR4nO3deXhc1WH38e+ZXftuSbZl492ywWAMNsEsYTEQMGAIJRBCSkjrJE0aKGmTtmlD+jRp3rRp3iQNSwhJaJqFkgCJwYTFLAbbeN93S15lW7Zk7ZoZjWbmvH9Y5nXAxpJnRmeCf5/n8fNYw3Duz5qr35x75upeY61FRETc8LgOICJyJlMJi4g4pBIWEXFIJSwi4pBKWETEIV8mBg3482woWJyJoUVE+m38mCOuI7xj975emlsS5t2PZ6SEQ8FiZkz5bCaGFhHpt5ee+bnrCO+Yfu2+Ez6u5QgREYdUwiIiDqmERUQcUgmLiDikEhYRcUglLCLikEpYRMQhlbCIiEMqYRERh1TCIiIOqYRFRBxSCYuIOKQSFhFxqF8lbIy5zhizzRhTZ4z5+0yHEhE5U5yyhI0xXuAh4CPAJOBOY8ykTAdzacPOZ9mx9xU6ug6QDXej3tW4hE275nGkrY5kMuE6Ds1tdayte4qDTeuIx6Ou4xCJtrJmx5PsOfg20Z4O13FI2gTr659mx75X6exuzIp9qO7Am2ze9Rwt7fUkrft96HDrVtbW/YbG5g3EEz2u47BjZ4yr7zzCD3/SxsFD8UHddn+uJzwdqLPW7gQwxjwJ3AxszmSwpE1gHRXOoUNrKJx+Kwe2PIMnkaSqpJYhJZPIyynHmPdckznjDrduJVkzjtZDi4nXP015aS3VJZMozB+Oxwz+ilJLez1d+UHCvbvYsmY+JcWjqSqupaxoDF5vYNDzdIYb6Uy005MTZefGR8jNraCquJYhJRMJ+PMGPU8i2UtT80YKzr+RA1ufwms9VJbUUlkyidxQqaN9aAueUWfTsv9N4nVPM6S0lqrSSRTkD8PjYFXySHs93UU57OzZzuY1z1FaPJaq4lpKi0bj9fjTtp1wONmv561c18OKbXlsaB3JV761ktoJOdxzq4+Pzs6nujIjl11/hznVu7Qx5jbgOmvtX/R9fTcww1r7hXc9by4wFyAUKJp2ybQvpRRs/bb/5XDLppTGSMXIrzyPTSboXD2ftoVPYOMxZ1kAhtz2IDljLiSycxVHXvwvEp3NTvPkjJ3OkI9+jd4jDRz5w/fp2b/FaR44+polol20vvY43RsW4PEGSCbcvW7H9qGOFc/S/tYvsYleZ1kAKu/6NsFhk4jUL6flxR+S6G51mifvnFmUX38fsabdHHnhe8Qa68DrB4ffp5FfeZ5EpIOWBT8ivHkhxaU5LHu+grGjUp9cTL92HyvXRd/zDpy2Ej5eYf4w+6d8Z40Fb3+NwvNuILxtMQFfLlXFtVSWTSYvp9xJnmVbf0ayagTxxnqSkU4qSydTVVJLUUENxsFMuG7vAhp9bZjeOLGm3ZSVjKeqpJayorF4vembxfRXc+sONh1cQLDiLMJ71lJYWEN1US0VpY5mwokYr6/4FgVTZhHZtoRgoODoPlQ6mdycskHPA/D2psdg+Dh692+DWITKkklUlR49mnIxM9+650Wag1FMNEyseS/lpRMgkeRQ8zqmTb6XksKz0rKd/t5Z45n5XdzzVQ855VV07drIhefnMWOK5TsPt/Hg35XxtQdKUs5yshLuzzx7P1Bz3NfD+x77wKqunk7OkSiVEz7prHiPV1UymUh7C1VDr3VWvMcrLRpD+MhaqoonUzb8o06K93j5uUMoCQ6hLDmEinO/6KR4j2c8Xqorzye3NU7lxE85K97jVZWdTaytk6qaG5wV7/HKC8fQ27qJqpJplI24HY/Hx+IND5Mz+gIaWzanrYT767yzg1w1rZObZjVw87XDKC3x8ukvtZA7Zhr/8+wOvvZA5rbdn5mwD9gOXMXR8l0BfNxae9K1gj/1mbCIDK7ucBMrdvyCIR//Fk2/+DKXnfdAWiYbp3uPuUTCUn72fvL/7Pu0P/UlVjxfyoSxqS1JnGwmfMp/pbU2DnwBeAnYAjz1fgUsIjJQjS2byJ04E3/ZcDy5xbR17nWa582lEXwFZfhLhxEaP5P/ndedsW31663GWvuCtXa8tXaMtfabGUsjImekw+3biTXtoeGhPydpLE1t253mefqFKJGYhwMPf5JYcwO/eSFzp63pN+ZExLmJw2dR2hMg0XWE0YXnMrximtM89306n/+4P0pvZwsfv3QfD38jP2PbUgmLiHMlRaOoKp8CQE3VdOcfZo4bHeBz9xQBcNdHC7j0opyMbUslLCLikEpYRMQhlbCIiEMqYRERh1TCIiIOqYRFRBxSCYuIOKQSFhFxSCUsIuKQSlhExCGVsIiIQyphERGHVMIiIg6phEVEHFIJi4g4pBIWEXFIJSwi4pBKWETEIZWwiIhDKmEREYdUwiIiDqmERUQcUgmLiDikEhYRcUglLCLikEpYRMShU5awMeanxpjDxpiNgxEoGzQcWklz63aSybjrKAA0t+3gQNNaeuMR11EA6I40s+fgEiI9ba6jABCPR9l9YBEdXQew1rqOg7VJ9jUu50hbHclkwnUcAJpat3GwaR3xeNR1FAA6w4fZe3Ap0Z4O11Gc8/XjOU8APwR+ntko2WPrznmEKsfTu/NZKkomUlVSS2nRaDye/ny70q++cRExv2Hr7hcoLh5FVXEtFSUT8PtynOQ52LSW/ZF6dh5cRE6olKriWirLJpMTLHaSp61zH7sOL2dPy1o8iSRVJbVUlk6mIK8aY8yg50km42zbPZ9QxRji9U9TXlpLdckkSgpH4fF4Bz0PQN2BhcRDQbbsmU9J0RiqiidSUTIBny/kJM/+ptUciu6j/sBCcnMrqCquJegvcJLFtVO2irX2TWPMWYOQ5Y/s2PMyew4sGuzNvqPynu8S72ime9NrrFvyJCYJyWTMWZ4htz1IcPhkwjveZvvCn7O57ndA0lmenPEfouLerxDds459i35N3eqX8Hh8To8ehn35OWKH6mleOY89Gx4lGCyip6fdWZ7KT32PePthujYsYO3SX2MwJBPu9qHKG79NoHwk4e1L2Lrwf9hU/zuw7mbq+VOuofSazxHdvZa9b/2Cnt31zrKciLUWjCfjR1dpm9oZY+YCcwFCgaKUxxs2ZBoBf37K45yOHXtepHvrIqKbFhLes5aiwpFUFo4jaRPA4M+sdux5iVjzHiI7lhHZtoRgoIARNVfg8waxdvCLeP/hlZhADu2LnySy+U2IRagZejF5oXISDkqmo6uB1kQLnaueI7r5TWLNe6isnEppXg1xB3mSNk793gV0b15IZNNCInvXU1w8miEFYxzuQy8SO1hHeP2rhLcvIRQqoapsGh6PHxj8JZx9jcvA46H9rV8R2fIWJh5jxLBLqCydPOhZTmbj1hjYJJu29XLJjMxtJ20lbK19DHgMoDB/WMqvam5OGSNzLk451+loaF5D/K2nqSmupeLcLxLw5znJcUxnTzOdq185etg/8VPk5pQ5zePz5VBf/walJbWMr7mBwvxhGOPuM97O7kZat/+SwMYVjCg+n7IRtztbOoKjyxH7m9eRWDKPkcW1VJx3JX5/rrM8AG2RRsKrXqS6aCKVk/6SnFCJ0zzG42PP9reP7kMjb6Ywb6iTpaP38+TvwwTKhvHEM2E+88nMbcf0Z6rdtxzxvLX27P4MWpg/zM6Y8tkUo7ljrc2qHSLb8kD2ZVKe95dteWBwMr30zOl9lGWt5ayLDtE78x9p+93X2b18KEPKU3tjn37tPlaui77nH6xT1E4g23bWbMsD2ZdJed5ftuWB7Mx0zMatMdq7vQRrzqFg7FSefaE7Y9vqzylqvwbeBiYYYxqMMZ/OWBoRkSzw5O/DJAtq6Fj+LFFvCU883ZuxbfXn7Ig7M7Z1EZEsdNG0EOu3H+SFP/yUsyfnMefGzJ0+5+7TCxGRLHXjrBxunJWDt/oID/9bMTOnZ+6cfK0Ji4g4pBIWEXFIJSwi4pBKWETEIZWwiIhDKmEREYdUwiIiDqmERUQcUgmLiDikEhYRcUglLCLiUEauHTF0dDP/8qufZWLo0+I32XGzxeOFsiyT37i7VdKJhIz7G3a+m991gHcJObyQ/on4sywPQK9N/VWL2yS9abgNlD3JHUyy77smInIGUQmLiDikEhYRcUglLCLikEpYRMQhlbCIiEMqYRERh1TCIiIOqYRFRBxSCYuIOKQSFhFxSCUsIuKQSlhExCGVsIiIQ6csYWNMjTHmdWPMZmPMJmPMfYMRzKX67THa27LnUpONBxM07I27jvGOSMSyaX0v1mbH5SattaxZEyMWy448AJu39NLenj2XB93XEGdfQ/bsQ51dSdZtjGXNPpRMWpavjtLbO/h5+nM94TjwJWvtamNMAbDKGPOKtXZzhrM58/HrDuEP+pk0NZfZczxcfk0ORcVeZ3nu/6sw2zaGqRmVw81zPFw7O8TwERm5FHS/PPZQNz9+JEJRqZ8bbvQz+yY/55zrxxjjJM+it2J86p4OAkEvV88KccscLzMvCRAIuMkTiViuu64Ff8jH+dNyuP0WwzXXBCkqcnfgece9EerrIowdl8Mdtxhunh2iZri7fehf/72bx3/eTXlFgNtv9nPbzUHOPdvdPjT/5TB3fOYIOTk+Zl+by123BPjwzJxB2bYZ6DuRMeb3wA+tta+c7DkTpwTtT+cNSynYEw+18ePvtKY0Ripq/uY3ROpXkKxfQGf9BkaND1K/uYuEo8nEkNu+Dl4f8brXCW9fQkWll0Q0ygFHs5v8KddQMO1Gera/QWzH6/gJM3wYbFgTdZLH4/NR/ZePE9m+CLtzAdHD+5lynp9li8NO8gDU3P8U4bplsHMBnTs3M36Cn43r3OWp/Pj/wcZjJOpeo3v7UoYP89LdHuVgo5ujvsLpt5JbexmxHQvp3f4GOf4oQ4ckWbuux0meQH4BFZ/4PpFtb8GuV+k50kh3Z4w35lVz0QWhlMe/+Lr9rFrX8553mQG9FRpjzgKmAstO8N/mAnMBKoemNmvs7kry0++1k5tnCAQH/52xrSVJsidMMtxGMtKO12coKzNEhvvo7EjPIeZA/lWtLUmsTWDD3RBtIRlPUFbux8YMkfDgz65aW5Lg9ZEIt0O0lXgkSsVQH+VlvZSUpiePOcldCE6kpcXiC4VIRjsg0kK8u5O8Ah9lpVBW5hn0Q97eXujstCR7wthIGzbSjs9vqCg3jKjx0tWdrn2o/3vRkZYkNhE/+ppFWrHJJGVlAQpCHifLOEdakuDxkAy3YyItxKNRSit9VJb3UpamfQj6/3PW3JLEGwyRiHQc3ae7Oyko9POJ23KZfn4wbXlOmLG/O6gxJh9YCHzTWvvM+z031Znwy7/v4hv/0M2EswP8+KmS0x7nmIHe3mjm+Aa8AR8zr8jl+jl+ZlwSSvubwUBub3T37Z1sXN3NedPzuXmO4YprcigqTm/5DuT2Rg9/v4sffreLURPymXOL4SM3BNO+PDKQ2xstfTvGnR9robwyhxtv9DPnZj/nTPGl/dC2vzfKiUYtEyc1EQj5mDUrxEfneLlkZvqXRwZye6MP39DJ1i1hZlyUz123Gq67JkRhYbr3of6P99VvdPHQo+1MnFzAXbd6mDM7RM2w9C+PBE3/XrWXXgsz5+5DVA7N4845QT52U4hzzw6kdR862Uy4XyVsjPEDzwMvWWu/e6rnp1rC99/bRr3/TroWPc4zb1ZRUpbazHqgJXzoYJySUm9GZ+EDKeGO9iTWkvbiPd5ASjgetxxqTDAsg2uKA73H3N49cWpGeDO6pjiQu5UdOJCgvNyT0XXpgZRwS2sSn5e0F+/xBlLCsZjlcFOC4Rko3uP1t4SttezaG2fUiPS/eR9zshLuz9kRBvgJsKU/BZyq7q4k65Z2kVt7GXljprLwpe5Mb/I9Kqt9TpZBTqawyJPRAh4on89ktIBPx4iRmfvhOR1Dh3qdfTB4IqUlnowW8EAFAibjBTwQxhhGj3TzwWB/XpWZwN3AlcaYtX1/rs9UoCWvhQlVjcTGIpjqacz/XfacKiYikm6nfCuy1i5iYJ8jpSQWM9iO/ex/5FMA1FxcPFibFhEZdNlzfNLnhtvyeHFlFQBf+VY5D/8y9Q/mRESyVdaVsIjImUQlLCLikEpYRMQhlbCIiEMqYRERh1TCIiIOqYRFRBxSCYuIOKQSFhFxSCUsIuKQSlhExKGMXEvOWkPUDuTqqyfWizct4zw4elrKY4jIn6CLprhO8I4d9Y+e8HHNhEVEHFIJi4g4pBIWEXFIJSwi4pBKWETEIZWwiIhDKmEREYdUwiIiDqmERUQcUgmLiDikEhYRcUglLCLikEpYRMQhlbCIiEOnLGFjTMgYs9wYs84Ys8kY8y+DEUxE5EzQn+sJ9wBXWmu7jDF+YJEx5g/W2qUZziYi8oF3ypmwPaqr70t/3x+b0VQiIlmgJ9aR8W30a03YGOM1xqwFDgOvWGuXneA5c40xK40xK9taEimFSsQtuQUBfN6UhhERSclbq75DR/fBjG6jXyVsrU1Ya88DhgPTjTFnn+A5j1lrL7DWXlBcmlp7rnk7TKQ7zqLX4imNIyKSqmQiltHxB3R2hLW2DXgduC4jafoseC5K4YU3s+rNTnqiyUxuSkTEqf6cHVFhjCnu+3sOMAvYmqlAibhl8ctd5E+dTc6wUSx/oztTmxIRca4/M+Fq4HVjzHpgBUfXhJ/PVKA1b4fxFw/BV1SJZ/TVvDwvs4cCIiIunfIUNWvtemDqIGQBYMemGN2H22n/9mw8Pi911aHB2rSIyKDLut+Yu+3eYn72h6EAfOKvinj02SrHiUREMifrStgfMAwdGQCgvNpPUYpnWoiIZLOsK2ERkTOJSlhExCGVsIiIQyphERGHVMIiIg6phEVEHFIJi4g4pBIWEXFIJSwi4pBKWETEof7cY27ALIaYTf3XjRM2PeN8fsf2lMdIt16bkW/9aYtav+sIf6Q3Da97ukWT2fU9evbeq1xHkDTQTFhExCGVsIiIQyphERGHVMIiIg6phEVEHFIJi4g4pBIWEXFIJSwi4pBKWETEIZWwiIhDKmEREYey6wIGIiJZoLP7IE0tWwHY27iU3niEitKJGdmWZsIiIu9y4Mh6GuL7CI2cQmdpAfWHFmdsWyphEZF3qSyeiOmJUnnHv+ELFlBVOCFj2+p3CRtjvMaYNcaY5zOWRkQkCxQV1JCMdBFr2k14x9tUlk3O2LYGsiZ8H7AFKMxQlpPq7kyw7NUuXvl9D5dcm8MNdxQMdoQ/Eu+1rF/SxcLnu/EHvXzhGxVO81hr2bUpwqL57ezc0svfPzSMUJ7b6/E2NfSw/A8trHo9zKcerKZmQq7TPOGOOGtebWHxvE4umVPMxTeXOc0TjyXZuqSVZfNbySsOcMdXRzjNY62lo2s/jS2b6O5p4dyxt+H1uL1+cjjawqEjm2ju2sWkEdeTl1M+aNs2xkNlaS1NLz9CTqiEnFBJxrbVrxI2xgwHbgC+CTyQsTTvsnVNhIUv9rJpRQf5I2uJecuwL67lgktDAxqnx9qUsyQSlgO7enjz+TDLF7QTKK2GqkuIb3uB2z9XNODx4jaRcqbWw70sfbmTRfM76Y0HCYy7nM7V89m7PULJkIH9APXYZMp5ot1x1i3sYNHzXTTti5I7/iJih3axcXE7OfkDe1PotamvlCXilrrVnSye18nONW3kjZpMLFnNqle2M/6CvAGP15NM7TVLJpIc2hlh2fxWNr7RTLB8OFRcCMsXMOueIQMeL9LTllIeay2xWCeHWrdwqHULBILk1F5Kx/JlhCPN+Hw5KY1/OuLxKM1tO2hs20I01k7u+Ivp6e6ltX0nHs/gnkdQkldDw46nqBlxTUa3Y2w/CsoY81vgW0AB8LfW2tkneM5cYC7AkKG+af/91rjTDmWt5dapu4l2RvGXj6DsI/cRHDqBtsW/pn3RL0973FQYXxCbiOEN5VNy9WfIrb2MeOtBDvz4M07yABiPwWIouuhjFF50Gx5/kD3ffs9LM2g8gSDJWA+5Ey6h9OrP4M0vcZoHjxeSCQKVoyn7yH0EKsfQsuBHdK56zk0efxB6e/DmFVFy9efInTCT3sM7OfjEfW7yHOPxUXzJxym88BaMz+/0NfMG8kjEusmbfAUlV/4F3twit/sQcPHU+8kNlaY8zrL1j9LRtd+8+/FTlrAxZjZwvbX2r4wxH+YkJXy88efk2B/8ftRph63bFOWvb9rFjGuKKavw89aLXRAsIh4ayvm1+/inHwzssCTVW/fEey2fuHA7BaVBrpqTxxvPddPeavEPPxdP01qeWDx6wGOm4/ZGbc29LH+pjdfnRdi7tZO8MefTVbea/3ppHGXVgQGNler3aN6jB/jtd/dxw9xq9u+0bFrcQu7wsfR0dHDPl4NcfOPAXrN03N4ombBsX9nB0vltrHqpBU9eKXF/OdMuPMK93xj44X8qtzeKRRP83UXLKK0pYOrVhax4oY1wJ/iqJxPo2sS/vnTugMdMx+2NemKdHDqymcb2LXR3N5Iz5kK6ty/hsqlfwu8f/CWkeKKH5tbtNLZtoaWtjlD1RHo7DlNbeTlDSmsHPU86nayE+9MEM4GbjDHXAyGg0BjzC2vtJ9Id8piF87vIGz+dTSs38OTSKj7/9TI2r47w+rx9TL1k8A+R1i/pIlhWTVdbM5fOLuCOL1bQUB9l0fx6rCke9DzHFJf7ueauCq6561ghN1C3uZSCksE//Xvxc13kjruIpgN7eODhkfSEh7PuzTZWvZZgzLn5g54HwOM1TJxRxMQZRXzywRF9hdzM+VcXD3qWzW+2kDN0FK0H93Hp7eO46b6RHNjRzcoX9hPMG/hSRLoEAwWMqJ7BiOoZ7xRyd+U0vN6gkzw+b5Cq8nOoKj/nnUJuDnopzKt2kmcw9Gs54p0nD8JM2FrL3Zc14L/6X+l+9d/55+8EOGd6au/Iqc7y/u/fNbKh/Ubo3M+1M9fysS+k/kPzQbrR56G9Uf7p1jrK7/ohTT//LI8sm4I/kNqa7gftRp8/uq+OBjOH5KFNXHntPq66Z3jKeXSjzz8tJ5sJZ915wju39BCJ+QlUjcU35kpefy7sNE+817J8QTu5Ey4hMO7DvDEv4jRPNlr2Qgs542fiK6wgNGQ4Gxe3u46UVWLRBFsWNZM7/mIC465g6fP6/sj/N6DpmLX2DeCNjCTps/z1bhJJLw3fv52cqhEs3RHmi5nc4ClsXxumtydJ82++ijcnl/C+dpoPxigf4JrrB9nKV8PEWnax9z9vIVhWyaoFSaZekblTev7U1K1sxyYNh3/5AL78YsJ7m+lu6yWv2O0pYJIdsm4mfO2fFfL5r/hJ9kSYMm4///DdwTs38ETGn5fL579ZRU/zAcL76vjbH4ygrEo/PMe752tVzLopjI33cvffBLjxM27Pm84246YXc+eDI4m1HCK8dxt/+YPJKmB5R9aVcOkQP7NuKwZg+pX5Ka8Hp8rnN1xxy9FZ3ZDhfmZeX4wx71nWOaONPiefGdcfPYXn0lsrqBwxsPO4P+j8AQ8XzakCYOQ5BZw3y+3EQrJL1pWwiMiZRCUsIuKQSlhExCGVsIiIQyphERGHVMIiIg6phEVEHFIJi4g4pBIWEXFIJSwi4pBKWETEoYxc1LZxVyn/+Yk7Uxzla7z8+Aw2PTctLZlSt56Opjweuvs210GyUlf4MLCBn//5Da6jZMzsnyxMeQyLybprSYtbmgmLiDikEhYRcUglLCLikEpYRMQhlbCIiEMqYRERh1TCIiIOqYRFRBxSCYuIOKQSFhFxSCUsIuKQSlhExCGVsIiIQyphERGH+nVNPWPMbqATSABxa+0FmQz1p6A3HqGpZSsYw9CK81zHyTrWWjq69tPcXsfwIdMIBgpcR8o63W29bHi1CV/QxwWzh7iOQyLZS0tbPd3RI4ys/hDGuJ2jWZukvauBI+311FROJ+DPc5onUwZyYdMrrLXNGUvyJ8DaJAcOr6GxbQtt7bsJVo4h0dqoEj5Oe2cDh1o209i6GQJBkjZBXrCUqooprqNlhUTcsvTpAyx/rpW9G1oJVo4ikGxwVsLHivdg22aOtGwnUDGSaOMOhlVMxe/PHfQ8x4q3sWUzh1s2Y0J5JOJRCnKrGFJaO+h5BoOx1p76SUdnwhf0t4QL84fZGVM+m1KwBW9/jam1n6SseGxK46TL0vWP0BU+hCeYR/GH7yFv4qUkwm0ceGyus0wTRs2mpmq6s+0fL9rTxqL1P4R4jNzxMymaeQf+irPY+x83g006y3XFjH/G6/GnZaxUL+r+jeuW0NwQxZdTQNHlnyZ3wsXEWw9w8In70pLvdHi8AZI2QdGHPkb+edfhyy9lz7dnO8sTCJUQi7aSV3s5hR+6nUDFSKd5AK6c8SAejzflcZatf5SOrv3m3Y/3dyZsgZeNMRb4kbX2sXc/wRgzF5gLEAoUpZIVgCum/xNebyDlcdJlau0naTqyicb2bbS9+jixupX4R54NwOSxH3WSqbx4nJPtnkgoWMy08XfR2LqVw3s30NK0h9CkywhWjqUkHqLMQdZQoDBtBZwOX/zFNNa9fJgV89s5+MbDxPctxZTVYjyGu/5t4LO8NY+nNjM8OgGzNHXUcWTZM/Tu2UBo8uUATBh1Az5vKKXxT4cxHlq79nJo52paDu8mZ9JlBMpGUu4po6Ro1KDnyQkWp6WA309/Z8LDrLX7jTFDgFeAv7bWvnmy56djJpzNYr3dNLVs4WDbFrzeAFPHfsx1pKxibZL2zn0cbNnMkY46zhk1h6KCGtexUpaO2xsd09ncw7oFTSx/ro2iigCf/t74AY/x/KcvT1ueRCJGc9sOGtu2EI40M732XqeTIGuTtHbs4VDrZo6072TK2NsozKt2licdTjYT7lcJ/9H/YMzXgS5r7XdO9pwPegnLmSmdJZwO6SxhybyTlfApP/40xuQZYwqO/R24BtiY/ogiImee/qwJVwLPGmOOPf9X1toXM5pKROQMccoSttbuBM4dhCwiImcc/caciIhDKmEREYdUwiIiDqmERUQcUgmLiDikEhYRcUglLCLikEpYRMQhlbCIiEMqYRERh1TCIiIOqYRFRBwa8PWE+zWoMU3AnjQMVQ5k033tlOf9ZVseyL5MyvP+si0PpC/TSGttxbsfzEgJp4sxZmU23dlZed5ftuWB7MukPO8v2/JA5jNpOUJExCGVsIiIQ9lewu+5q7NjyvP+si0PZF8m5Xl/2ZYHMpwpq9eERUQ+6LJ9Jiwi8oGmEhYRcSgrS9gYc50xZpsxps4Y8/dZkOenxpjDxpiNrrMAGGNqjDGvG2M2G2M2GWPuc5wnZIxZboxZ15fnX1zmOcYY4zXGrDHGPO86C4AxZrcxZoMxZq0xZmUW5Ck2xvzWGLPVGLPFGPMhh1km9H1fjv3pMMbc7ypPX6a/6dufNxpjfm2MCWVkO9m2JmyM8QLbgVlAA7ACuNNau9lhpsuALuDn1tqzXeU4Lk81UG2tXW2MKQBWAXNcfY+MMQbIs9Z2GWP8wCLgPmvtUhd5jsv1AHABUGitne0yS1+e3cAF1tqs+GUEY8x/A29Zax83xgSAXGttm+NYxzpgPzDDWpuOX/o6nQzDOLofT7LWRowxTwEvWGufSPe2snEmPB2os9butNbGgCeBm10Gsta+CbS4zHA8a+1Ba+3qvr93AluAYQ7zWGttV9+X/r4/Tt/djTHDgRuAx13myFbGmCLgMuAnANbaWDYUcJ+rgHpXBXwcH5BjjPEBucCBTGwkG0t4GLDvuK8bcFgw2c4YcxYwFVjmOIfXGLMWOAy8Yq11mgf4HvBlIOk4x/Es8LIxZpUxZq7jLKOAJuBnfUs2jxtj8hxnOuYO4NcuA1hr9wPfAfYCB4F2a+3LmdhWNpaw9JMxJh94GrjfWtvhMou1NmGtPQ8YDkw3xjhbtjHGzAYOW2tXucpwEpdYa88HPgJ8vm+ZyxUfcD7wiLV2KtANZMPnLwHgJuA3jnOUcPQIfBQwFMgzxnwiE9vKxhLeD9Qc9/XwvsfkOH1rr08Dv7TWPuM6zzF9h7SvA9c5jDETuKlvDfZJ4EpjzC8c5gHemV1hrT0MPMvRpTdXGoCG445YfsvRUnbtI8Bqa+0hxzmuBnZZa5ustb3AM8DFmdhQNpbwCmCcMWZU37viHcA8x5mySt8HYT8Btlhrv5sFeSqMMcV9f8/h6IeqW13lsdb+g7V2uLX2LI7uP69ZazMyi+kvY0xe34eo9B32XwM4O9vGWtsI7DPGTOh76CrA2Yffx7kTx0sRffYCFxljcvt+3q7i6GcvaefLxKCpsNbGjTFfAF4CvMBPrbWbXGYyxvwa+DBQboxpAB601v7EYaSZwN3Ahr51WIB/tNa+4ChPNfDffZ9qe4CnrLVZcVpYFqkEnj3684wP+JW19kW3kfhr4Jd9k52dwKdchul7c5oFfMZlDgBr7TJjzG+B1UAcWEOGfn05605RExE5k2TjcoSIyBlDJSwi4pBKWETEIZWwiIhDKmEREYdUwiIiDqmERUQc+n9qUaJYmRKn4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent4  = PrioritizedSweep(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, theta=0.0001)\n",
    "reward4 = train(agent4, world, switch=switch, remove=remove)\n",
    "agent4.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "* DynaQ and Prioritized-Sweeping don't find the newly opened shortcut, while both DynaQ+ and DynaQ+E do. \n",
    "* DynaQ+Es state-action space is much _cleaner_ than that of DynaQ+.\n",
    "* The state-action space of Prioritized-Sweeping seems to be optimal with the exception of the new shortcut.\n",
    "\n",
    "## Cumulative Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum(reward):\n",
    "    '''calculate weighted cumulative rewards given by training function'''\n",
    "    return np.cumsum([i[0] / i[1] for i in reward])\n",
    "\n",
    "def plot(episodes, rewards, labels, switch):\n",
    "    '''plot cumulative rewards for different agents'''\n",
    "    x       = np.arange(episodes)\n",
    "    cumsums = [cumsum(reward) for reward in rewards]\n",
    "    largest_reward = max([cs[-1] for cs in cumsums])\n",
    "    \n",
    "    for i in range(len(cumsums)):\n",
    "        plt.plot(x, cumsums[i], label=labels[i])\n",
    "    plt.vlines(switch, 0, largest_reward, linestyles='dashed', label='Shortcut open')\n",
    "    plt.xlabel('Episode No.')\n",
    "    plt.ylabel('Average Cumulative Reward')\n",
    "    plt.title('Comparison of Cumulative Rewards over time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(episodes, \n",
    "     [reward1, reward2, reward3, reward4], \n",
    "     ['DynaQ', 'DynaQ+', 'DynaQ+E', 'PrioSweep'], \n",
    "     switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 2\n",
    "Old path closes up at 1000 episodes, new one opens up on other side for 2000 more episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 3000\n",
    "moves    = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "world    = shortcut\n",
    "switch   = 1000\n",
    "add      = [(3, 0)]\n",
    "remove   = [(3, 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward1 = train(agent1, world, switch=switch, add=add, remove=remove)\n",
    "agent1.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward2 = train(agent2, world, switch=switch, add=add, remove=remove)\n",
    "agent2.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward3 = train(agent3, world, switch=switch, add=add, remove=remove)\n",
    "agent3.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward4 = train(agent4, world, switch=switch, add=add, remove=remove)\n",
    "agent4.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(episodes, \n",
    "     [reward1, reward2, reward3, reward4], \n",
    "     ['DynaQ', 'DynaQ+', 'DynaQ+E', 'PrioSweep'], \n",
    "     switch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
