{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Environment Tests\n",
    "Gridworld where a shortcut opens up halfway through. \n",
    "\n",
    "With agents that learn __and__ plan\n",
    "\n",
    "\n",
    "* Position in the gridworld is the state\n",
    "* Actions are compass moves between positions.\n",
    "* Reward is 0 everywhere, except when reaching the goal, where it is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    '''Class defining the environment'''\n",
    "    def __init__(self, shapex, shapey, walls, start, goal):\n",
    "        self.shapex = shapex\n",
    "        self.shapey = shapey\n",
    "        self.walls  = walls\n",
    "        self.start  = start\n",
    "        self.goal   = goal\n",
    "        self.idx    = self.build_index()\n",
    "        self.size   = shapex * shapey\n",
    "        \n",
    "        self.set_bitmap()\n",
    "        self.original_walls = walls.copy()\n",
    "        \n",
    "    def reset_walls(self):\n",
    "        self.walls  = self.original_walls\n",
    "        self.set_bitmap()\n",
    "        \n",
    "    def state_from_index(self, number):\n",
    "        '''get state from index'''\n",
    "        x = number // self.shapey\n",
    "        y = number  % self.shapey\n",
    "        \n",
    "        return (x, y)\n",
    "        \n",
    "    def build_index(self):\n",
    "        '''build index for indexing states'''\n",
    "        return {(x, y) : self.shapey * x + y for x in range(self.shapex) \n",
    "                for y in range(self.shapey)}\n",
    "        \n",
    "    def set_bitmap(self):\n",
    "        '''builds the bitmap for visualisation'''\n",
    "        bitmap = np.ones((self.shapex, self.shapey))\n",
    "        \n",
    "        for wall in self.walls:\n",
    "            bitmap[wall] = 0\n",
    "        \n",
    "        self.bitmap = bitmap\n",
    "    \n",
    "    def show_bitmap(self):\n",
    "        '''shows the environment of holes and walkable path'''\n",
    "        plt.imshow(self.bitmap)\n",
    "        \n",
    "    def change(self, add=[], remove=[]):\n",
    "        '''add or remove walls'''\n",
    "        self.walls.extend([i for i in add])\n",
    "        self.walls = [wall for wall in self.walls if wall not in remove]\n",
    "        self.set_bitmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shapex = 6\n",
    "shapey = 9\n",
    "start  = (5, 5)\n",
    "goal   = (0, 8)\n",
    "walls  = [(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8)]\n",
    "\n",
    "shortcut = Maze(shapex, shapey, walls, start, goal)\n",
    "shortcut.show_bitmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "### Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95):\n",
    "        self.maze       = maze\n",
    "        self.moves      = moves\n",
    "        self.plan_steps = plan_steps\n",
    "        self.alpha      = alpha\n",
    "        self.epsilon    = epsilon\n",
    "        self.gamma      = gamma\n",
    "        self.move_idx   = {moves[i] : i for i in range(len(moves))}\n",
    "    \n",
    "    # -------------------------------------------Methods-------------------------------------------------#\n",
    "    \n",
    "    def reset(self):\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.history = [set()] * self.maze.size # History for planning part\n",
    "        self.history_full = np.zeros(self.maze.size, dtype=bool)\n",
    "        \n",
    "            \n",
    "    def step(self, S, A):\n",
    "        Sn, R = self.move(S, A)\n",
    "\n",
    "        # Only add to History if it isnt full yet\n",
    "        if not self.history_full[self.maze.idx[S]]:\n",
    "            self.history[self.maze.idx[S]].add(A)\n",
    "            if len(self.history[self.maze.idx[S]]) == 4:\n",
    "                self.history_full[self.maze.idx[S]] = 1\n",
    "                \n",
    "        self.visited = [i for i, h in enumerate(self.history) if h]\n",
    "        return Sn, R\n",
    "        \n",
    "    def q_update(self, S, A, R, Sn):\n",
    "        Am       = np.argmax(self.Q[Sn])\n",
    "        idx      = (S[0],  S[1],  A)\n",
    "        idxn     = (Sn[0], Sn[1], Am)\n",
    "        self.Q[idx] += self.alpha * (R + self.gamma * self.Q[idxn] - self.Q[idx])\n",
    "        \n",
    "    def m_update(self, S, A, R, Sn):\n",
    "        idx         = (S[0], S[1], A)\n",
    "        self.M[idx] = R, Sn[0], Sn[1]\n",
    "        \n",
    "    def update(self, S, A, R, Sn):\n",
    "        self.q_update(S, A, R, Sn)\n",
    "        self.m_update(S, A, R, Sn)\n",
    "        \n",
    "    def plan(self):\n",
    "        for step in range(self.plan_steps):\n",
    "            Spi = random.choice(self.visited)\n",
    "            Sp  = self.maze.state_from_index(Spi)\n",
    "            Ap  = random.choice(tuple(self.history[Spi]))\n",
    "            Rp  = self.M[Sp[0], Sp[1], Ap, 0]\n",
    "            Spn = tuple(self.M[Sp[0], Sp[1], Ap, 1:])\n",
    "            \n",
    "            self.q_update(Sp, Ap, Rp, Spn)\n",
    "            \n",
    "    def done(self, S):\n",
    "        return S == self.maze.goal\n",
    "    \n",
    "    # --------------------------------------------Setup--------------------------------------------------#\n",
    "    \n",
    "    def move(self, S, A):\n",
    "        # Clamp values to map\n",
    "        direction = self.moves[A]\n",
    "        \n",
    "        Sx  = int(max(0, min(self.maze.shapex - 1, S[0] + direction[0])))\n",
    "        Sy  = int(max(0, min(self.maze.shapey - 1, S[1] + direction[1])))\n",
    "        Sn = (Sx, Sy)\n",
    "\n",
    "        # Reset position if falling in hole\n",
    "        if Sn in self.maze.walls:\n",
    "            Sn = S\n",
    "\n",
    "        R = 1 if Sn == self.maze.goal else 0\n",
    "\n",
    "        return Sn, R\n",
    "    \n",
    "    def greedy(self, S):\n",
    "        return np.argmax(self.Q[S])\n",
    "    \n",
    "    def epsilon_greedy(self, S):\n",
    "        return np.argmax(self.Q[S]) if np.random.rand() > self.epsilon else np.random.randint(len(moves))\n",
    "    \n",
    "    def trace(self):\n",
    "        S = self.maze.start\n",
    "        path = []\n",
    "\n",
    "        while not self.done(S):\n",
    "            path.append(S)\n",
    "            A    = self.greedy(S)\n",
    "            S, _ = self.step(S, A)\n",
    "\n",
    "            # Loop detection\n",
    "            if S in path:\n",
    "                print(\"State Values dont create greedy path\")\n",
    "                break\n",
    "\n",
    "        return path\n",
    "    \n",
    "    # -------------------------------------------Imaging-------------------------------------------------#\n",
    "    \n",
    "    def create_best_path_img(self):\n",
    "        path = self.trace()\n",
    "        \n",
    "        c0 = 5\n",
    "        c1 = 6\n",
    "        \n",
    "        length    = len(path)\n",
    "        grid      = self.maze.bitmap.copy()\n",
    "        fade      = np.linspace(c0, c1, num=length)\n",
    "        \n",
    "        for i in range(length):\n",
    "            grid[path[i]] = fade[i]\n",
    "            \n",
    "        grid[self.maze.start] = c0\n",
    "        grid[self.maze.goal]  = c1\n",
    "        \n",
    "        print(f\"path length: {length}\")\n",
    "        return grid\n",
    "    \n",
    "    def create_direction_map_img(self):\n",
    "        arr_len    = 0.85\n",
    "        grid       = self.create_best_path_img()\n",
    "        directions = [(S, self.greedy(S))\n",
    "                      for x in range(self.maze.shapex) \n",
    "                      for y in range(self.maze.shapey) if (S := (x, y))]   \n",
    "\n",
    "        for i in directions:\n",
    "            if i[0] not in self.maze.walls and i[0] != self.maze.goal:\n",
    "                    dx = arr_len if i[1] == 2 else -arr_len if i[1] == 1 else 0 \n",
    "                    dy = arr_len if i[1] == 3 else -arr_len if i[1] == 0 else 0\n",
    "                    plt.arrow(i[0][1], i[0][0], dx, dy, head_width=0.1)\n",
    "\n",
    "        return grid\n",
    "    \n",
    "    def draw_path_and_dir_map(self):\n",
    "        plt.imshow(self.create_direction_map_img())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+\n",
    "Additional variable $\\tau$ tracking for each state how many timesteps have gone by since it was last visited.\n",
    "\n",
    "In the planning phase we add $k \\cdot \\sqrt{\\tau}$ to the planned reward. This changes the estimated values for states and actions but _encourages_ exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlus(DynaQ):\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=0.00001):\n",
    "        super().__init__(maze, moves, plan_steps, alpha, epsilon, gamma)\n",
    "        self.k = k\n",
    "        \n",
    "    def reset(self):\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.T = np.zeros(self.maze.size)\n",
    "        self.history      = [set()] * self.maze.size # History for planning part\n",
    "        self.history_full = np.zeros(self.maze.size)\n",
    "            \n",
    "    def step(self, S, A):\n",
    "        Sn, R = self.move(S, A)\n",
    "        self.T[self.maze.idx[S]] = 0\n",
    "\n",
    "        # Only add to History if it isnt full yet\n",
    "        if not self.history_full[self.maze.idx[S]]:\n",
    "            self.history[self.maze.idx[S]].add(A)\n",
    "            if len(self.history[self.maze.idx[S]]) == 4:\n",
    "                self.history_full[self.maze.idx[S]] = 1\n",
    "                \n",
    "        self.visited = [i for i, h in enumerate(self.history) if h]\n",
    "        \n",
    "        for i in self.visited:\n",
    "            self.T[i] += 1\n",
    "            \n",
    "        return Sn, R\n",
    "                \n",
    "    def plan(self):\n",
    "        for step in range(self.plan_steps):\n",
    "            Spi = random.choice(self.visited)\n",
    "            Sp  = self.maze.state_from_index(Spi)\n",
    "            Ap  = random.choice(tuple(self.history[Spi]))\n",
    "            Rp  = self.M[Sp[0], Sp[1], Ap, 0] + self.k * np.sqrt(self.T[Spi])\n",
    "            Spn = tuple(self.M[Sp[0], Sp[1], Ap, 1:])\n",
    "            \n",
    "            self.q_update(Sp, Ap, Rp, Spn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+Experiment (Ex. 8.4)\n",
    "Instead of adding $k \\cdot \\sqrt{\\tau}$ to the planned reward, we change our greedy action to be greedy towards the reward $R + k \\cdot \\sqrt{\\tau}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlusExperiment(DynaQ):\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=0.00001):\n",
    "        super().__init__(maze, moves, plan_steps, alpha, epsilon, gamma)\n",
    "        self.k = k\n",
    "        \n",
    "    def reset(self):\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.T = np.zeros(self.maze.size)\n",
    "        self.history      = [set()] * self.maze.size # History for planning part\n",
    "        self.history_full = np.zeros(self.maze.size)\n",
    "            \n",
    "    def step(self, S, A):\n",
    "        Sn, R = self.move(S, A)\n",
    "        self.T[self.maze.idx[S]] = 0\n",
    "\n",
    "        # Only add to History if it isnt full yet\n",
    "        if not self.history_full[self.maze.idx[S]]:\n",
    "            self.history[self.maze.idx[S]].add(A)\n",
    "            if len(self.history[self.maze.idx[S]]) == 4:\n",
    "                self.history_full[self.maze.idx[S]] = 1\n",
    "                \n",
    "        self.visited = [i for i, h in enumerate(self.history) if h]\n",
    "        \n",
    "        for i in self.visited:\n",
    "            self.T[i] += 1\n",
    "        \n",
    "        return Sn, R\n",
    "    \n",
    "    def epsilon_greedy(self, S):                \n",
    "        i = np.argmax([self.Q[Sn][i] + self.k * np.sqrt(self.T[self.maze.idx[Sn]]) \n",
    "                       for i in range(len(self.moves)) \n",
    "                       if (Sn := self.move(S, i)[0])])\n",
    "        \n",
    "        return i if np.random.rand() > self.epsilon else np.random.randint(len(moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Sweeping\n",
    "In DynaQ we plan updates from a random sampling of states, in Prioritized Sweeping we have a queue which prioritizes updates by size of the update. Parameter $\\theta$ determines the minimum size of an update.\n",
    "\n",
    "The updates then start at the biggest change and work their way to the smallest change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedSweep(DynaQ):\n",
    "    def __init__(self, maze, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, theta=0.1):\n",
    "        super().__init__(maze, moves, plan_steps, alpha, epsilon, gamma)\n",
    "        self.theta = theta\n",
    "    \n",
    "    def reset(self):\n",
    "        self.Q = np.random.rand(self.maze.shapex, self.maze.shapey, len(moves)) * 5\n",
    "        self.M = np.zeros((self.maze.shapex, self.maze.shapey, len(moves), 3), dtype=int)\n",
    "        self.PQueue = []\n",
    "        \n",
    "    def p_update(self, S, A, R, Sn):\n",
    "        Am       = np.argmax(self.Q[Sn])\n",
    "        idx      = (S[0],  S[1],  A)\n",
    "        idxn     = (Sn[0], Sn[1], Am)\n",
    "        P        = abs(R + self.gamma * self.Q[idxn] - self.Q[idx])\n",
    "        if P > self.theta:\n",
    "            self.insert_correctly(P, S, A)\n",
    "            \n",
    "    def step(self, S, A):\n",
    "        Sn, R = self.move(S, A)\n",
    "        return Sn, R\n",
    "    \n",
    "    def update(self, S, A, R, Sn):\n",
    "        self.m_update(S, A, R, Sn)\n",
    "        self.p_update(S, A, R, Sn)\n",
    "        \n",
    "    def plan(self):\n",
    "        for step in range(self.plan_steps):\n",
    "            if len(self.PQueue) > 0:\n",
    "                _, S, A = self.PQueue.pop(0)\n",
    "                idx     = (S[0],  S[1],  A)\n",
    "                R       = self.M[idx][0]\n",
    "                Sn      = tuple(self.M[idx][1:])\n",
    "                self.q_update(S, A, R, Sn)\n",
    "                \n",
    "                # Catch all states leading to S according to Model M\n",
    "                MP = []\n",
    "                for x in range(self.maze.shapex):\n",
    "                    for y in range(self.maze.shapey):\n",
    "                        for a in range(len(self.moves)):\n",
    "                            if S == tuple(self.M[x, y, a][1:]):\n",
    "                                D = self.M[x, y, a]\n",
    "                                MP.append((D[0], D[1], D[2], a))\n",
    "                                \n",
    "                for i in range(len(MP)):\n",
    "                    Rp, Spx, Spy, Ap = MP[i]\n",
    "                    self.p_update((Spx, Spy), Ap, Rp, S)\n",
    "    \n",
    "    def insert_correctly(self, P, S, A):\n",
    "        to_insert = (P, S, A)\n",
    "        inserted  = False\n",
    "        \n",
    "        for i in range(len(self.PQueue)):\n",
    "            if P >= self.PQueue[i][0]:\n",
    "                self.PQueue.insert(i, to_insert)\n",
    "                inserted = True\n",
    "                    \n",
    "        if not inserted:\n",
    "            self.PQueue.append(to_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Shortcut world, shortcut opens when we're halfway through the learning process.\n",
    "If the agent doesn't reach the goal after 50 episodes I cut it off without reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 6000\n",
    "moves    = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "world    = shortcut\n",
    "switch   = 3000\n",
    "remove   = [(3, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, world, switch=-1, add=[], remove=[]):\n",
    "    agent.reset()    \n",
    "    world.reset_walls()\n",
    "    reward_history = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        S = world.start\n",
    "        t = 0\n",
    "\n",
    "        if episode == switch:\n",
    "            world.change(add=add, remove=remove)\n",
    "\n",
    "        while True:\n",
    "            A     = agent.epsilon_greedy(S)\n",
    "            Sn, R = agent.step(S, A)\n",
    "            agent.update(S, A, R, Sn)\n",
    "\n",
    "            S = Sn\n",
    "            agent.plan()\n",
    "            \n",
    "            t += 1\n",
    "            if agent.done(S) or t > 50:\n",
    "                reward_history.append((R, t))\n",
    "                break\n",
    "\n",
    "    return np.array(reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent   = DynaQ(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95)\n",
    "reward1 = train(agent, world, switch=switch, remove=remove)\n",
    "agent.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+ Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2  = DynaQPlus(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=10e-3)\n",
    "reward2 = train(agent2, world, switch=switch, remove=remove)\n",
    "agent2.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+Experiment Run\n",
    "(takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent3  = DynaQPlusExperiment(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, k=0.001)\n",
    "reward3 = train(agent3, world, switch=switch, remove=remove)\n",
    "agent3.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priorized Sweeping Run\n",
    "The prioritization of updates leads less timesteps to converge to the optimal path. \n",
    "But we run 6k episodes just like all of the other methods, so we only see the speedup in the graph later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent4  = PrioritizedSweep(world, moves, plan_steps=50, alpha=0.1, epsilon=0.1, gamma=0.95, theta=0.00001)\n",
    "reward4 = train(agent4, world, switch=switch, remove=remove)\n",
    "agent4.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "DynaQ doesn't find the newly opened shortcut, while DynaQ+ and DynaQ+E both do. \n",
    "\n",
    "## Cumulative Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum(reward):\n",
    "    return np.cumsum([i[0] / i[1] for i in reward])\n",
    "\n",
    "def plot(episodes, rewards, labels, switch):\n",
    "    x       = np.arange(episodes)\n",
    "    cumsums = [cumsum(reward) for reward in rewards]\n",
    "    largest_reward = max([cs[-1] for cs in cumsums])\n",
    "    \n",
    "    for i in range(len(cumsums)):\n",
    "        plt.plot(x, cumsums[i], label=labels[i])\n",
    "    plt.vlines(switch, 0, largest_reward, linestyles='dashed', label='Shortcut open')\n",
    "    plt.xlabel('Episode No.')\n",
    "    plt.ylabel('Average Cumulative Reward')\n",
    "    plt.title('Comparison of Cumulative Rewards over time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(episodes, [reward1, reward2, reward3, reward4], ['DynaQ', 'DynaQ+', 'DynaQ+E', 'PrioSweep'], switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 2\n",
    "Old path closes up at 1000 episodes, new one opens up on other side for 2000 more episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 3000\n",
    "moves    = [(-1, 0), (0, -1), (0, 1), (1, 0)]\n",
    "world    = shortcut\n",
    "switch   = 1000\n",
    "add      = [(3, 0)]\n",
    "remove   = [(3, 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward1 = train(agent1, world, switch=switch, add=add, remove=remove)\n",
    "agent1.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward2 = train(agent2, world, switch=switch, add=add, remove=remove)\n",
    "agent2.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaQ+Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward3 = train(agent3, world, switch=switch, add=add, remove=remove)\n",
    "agent3.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward4 = train(agent4, world, switch=switch, add=add, remove=remove)\n",
    "agent4.draw_path_and_dir_map();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(episodes, [reward1, reward2, reward3, reward4], ['DynaQ', 'DynaQ+', 'DynaQ+E', 'PrioSweep'], switch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
