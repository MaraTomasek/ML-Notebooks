{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. On Policy Prediction with Approximation\n",
    "## CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "[CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "### State (Observation)\n",
    "| num | observation          | min      | max     |\n",
    "|----:|:---------------------|---------:|--------:|\n",
    "| 0   | Cart Position        | -2.4     | 2.4     |\n",
    "| 1   | Cart Velocity        | -Inf     | Inf     |\n",
    "| 2   | Pole Angle           | ~ -41.8° | ~ 41.8° |\n",
    "| 3   | Pole Velocity at Tip | -Inf     | Inf     |\n",
    "\n",
    "### Action\n",
    "| num | action          | \n",
    "|----:|:----------------|\n",
    "| 0   | Push cart left  |\n",
    "| 1   | Push cart right |\n",
    "\n",
    "### Reward\n",
    "1 for every step the pole is upright, including termination. <br />\n",
    "\n",
    "### Termination\n",
    "* Pole angle more than ±12°\n",
    "* Cart Position more than ±2.4\n",
    "* Episode length > 200\n",
    "\n",
    "### Solved\n",
    "Average reward ≥195.0 over 100 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "based on ruippeixotog's solution on openai.com: https://gym.openai.com/evaluations/eval_aCiCDmwhTCytFuxMpKoyvQ/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import abc\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymRunner:\n",
    "    def __init__(self, env_id, max_timesteps=200):\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.env = gym.make(env_id)\n",
    "\n",
    "    def train(self, agent, num_episodes):\n",
    "        self.run(agent, num_episodes, do_train=True)\n",
    "\n",
    "    def train_until_solved(self, agent):\n",
    "        loop          = 0\n",
    "        trained       = False\n",
    "        training_eps  = 100\n",
    "        testing_stops = [10,  15,  25,  25,  25]\n",
    "        testing_evals = [150, 180, 190, 193, 195]\n",
    "        \n",
    "        while not trained:\n",
    "            train_rew = np.mean(self.run(agent, training_eps, do_train=True))\n",
    "            avg_rew = 0\n",
    "            \n",
    "            for stop in range(len(testing_stops)):\n",
    "                avg_rew = (np.mean(self.run(agent, testing_stops[stop])) + avg_rew) / 2\n",
    "                if avg_rew < testing_evals[stop]:\n",
    "                    break\n",
    "                elif testing_evals[stop] == 195 and avg_rew > testing_evals[stop]:\n",
    "                    print(f\"Model fully learned after {loop * trainin_eps} Episodes\")\n",
    "                    return 0\n",
    "            \n",
    "            loop += 1\n",
    "            print(f\"Episode {loop*training_eps}\\t Average Reward {train_rew}\")\n",
    "            \n",
    "        \n",
    "    def run(self, agent, num_episodes, do_train=False):\n",
    "        rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "            total_reward = 0\n",
    "\n",
    "            for t in range(self.max_timesteps):\n",
    "                action = agent.select_action(state, do_train)\n",
    "\n",
    "                # execute the selected action\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = next_state.reshape(1, self.env.observation_space.shape[0])\n",
    "                #reward = self.calc_reward(state, action, reward, next_state, done)\n",
    "\n",
    "                # record the results of the step\n",
    "                if do_train:\n",
    "                    agent.record(state, action, reward, next_state, done)\n",
    "\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            # train the agent based on a sample of past experiences\n",
    "            if do_train:\n",
    "                agent.replay()\n",
    "                \n",
    "            return rewards\n",
    "            \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # hyperparameters\n",
    "        self.gamma = 0.95  # discount rate on future rewards\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_decay = 0.995  # the decay of epsilon after each training batch\n",
    "        self.epsilon_min = 0.1  # the minimum exploration rate permissible\n",
    "        self.batch_size = 32  # maximum size of the batches sampled from memory\n",
    "\n",
    "        # agent state\n",
    "        self.model = self.build_model()\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def build_model(self):\n",
    "        return None\n",
    "\n",
    "    def select_action(self, state, do_train=True):\n",
    "        if do_train and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def record(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAgent(QLearningAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__(4, 2)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, activation='relu', input_dim=4))\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(2))\n",
    "        model.compile(Adam(lr=1e-3), 'mse')\n",
    "\n",
    "        # load the weights of the model if reusing previous training session\n",
    "        model.load_weights(\"models/cartpole-v0.h5\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep-Q-Network Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\t Average Reward 13.0\n",
      "Episode 200\t Average Reward 21.0\n",
      "Episode 300\t Average Reward 19.0\n",
      "Episode 400\t Average Reward 17.0\n",
      "Episode 500\t Average Reward 19.0\n",
      "Episode 600\t Average Reward 14.0\n",
      "Episode 700\t Average Reward 29.0\n",
      "Episode 800\t Average Reward 17.0\n",
      "Episode 900\t Average Reward 30.0\n",
      "Episode 1000\t Average Reward 11.0\n",
      "Episode 1100\t Average Reward 10.0\n",
      "Episode 1200\t Average Reward 14.0\n",
      "Episode 1300\t Average Reward 34.0\n",
      "Episode 1400\t Average Reward 28.0\n",
      "Episode 1500\t Average Reward 14.0\n",
      "Episode 1600\t Average Reward 40.0\n",
      "Episode 1700\t Average Reward 22.0\n",
      "Episode 1800\t Average Reward 36.0\n",
      "Episode 1900\t Average Reward 36.0\n",
      "Episode 2000\t Average Reward 18.0\n",
      "Episode 2100\t Average Reward 15.0\n",
      "Episode 2200\t Average Reward 32.0\n",
      "Episode 2300\t Average Reward 13.0\n",
      "Episode 2400\t Average Reward 18.0\n",
      "Episode 2500\t Average Reward 12.0\n",
      "Episode 2600\t Average Reward 29.0\n",
      "Episode 2700\t Average Reward 18.0\n",
      "Episode 2800\t Average Reward 12.0\n",
      "Episode 2900\t Average Reward 17.0\n",
      "Episode 3000\t Average Reward 26.0\n",
      "Episode 3100\t Average Reward 42.0\n",
      "Episode 3200\t Average Reward 18.0\n",
      "Episode 3300\t Average Reward 23.0\n",
      "Episode 3400\t Average Reward 21.0\n",
      "Episode 3500\t Average Reward 11.0\n",
      "Episode 3600\t Average Reward 38.0\n",
      "Episode 3700\t Average Reward 19.0\n",
      "Episode 3800\t Average Reward 18.0\n",
      "Episode 3900\t Average Reward 23.0\n",
      "Episode 4000\t Average Reward 12.0\n",
      "Episode 4100\t Average Reward 33.0\n",
      "Episode 4200\t Average Reward 9.0\n",
      "Episode 4300\t Average Reward 63.0\n",
      "Episode 4400\t Average Reward 42.0\n",
      "Episode 4500\t Average Reward 26.0\n",
      "Episode 4600\t Average Reward 18.0\n",
      "Episode 4700\t Average Reward 16.0\n",
      "Episode 4800\t Average Reward 13.0\n"
     ]
    }
   ],
   "source": [
    "gym   = GymRunner('CartPole-v0')\n",
    "agent = CartPoleAgent()\n",
    "gym.train_until_solved(agent)\n",
    "\n",
    "agent.model.save_weights(\"models/cartpole-v0.h5\", overwrite=True)\n",
    "gym.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
