{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex8.8 Trajectory Sampling Exercise\n",
    "\n",
    "## 0. Problem Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* undiscounted episodic tasks\n",
    "* 2 actions from each state, each resulting in one of $b$ states, all equally likely\n",
    "* different random selection of $b$ states for each state-action pair\n",
    "* for each transition theres a $0.1$ chance of entering a terminal state\n",
    "* expected reward for each transition sampled from $\\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, state_amount, actions, branching_factor):\n",
    "        self.states   = state_amount\n",
    "        self.actions  = actions\n",
    "        self.branches = branching_factor\n",
    "        \n",
    "        self.build_transition_graph()\n",
    "        \n",
    "    def build_transition_graph(self):\n",
    "        '''build neighbors for each state randomly'''\n",
    "        self.trans = np.random.randint(self.states, size=(self.states, self.actions, self.branches))\n",
    "    \n",
    "    def get_start(self):\n",
    "        '''Return the starting position (arbitrary but constant)'''\n",
    "        return 0\n",
    "    \n",
    "    def get_reward(self):\n",
    "        '''Returns reward as defined'''\n",
    "        return np.random.normal()\n",
    "    \n",
    "    def get_random_branch(self):\n",
    "        '''Choose a random branch'''\n",
    "        return np.random.randint(self.branches)\n",
    "\n",
    "    def is_terminal(self):\n",
    "        '''Call to determine if move is terminal'''\n",
    "        return np.random.rand() < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One Step Tabular Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularPlanning:\n",
    "    \n",
    "    def __init__(self, environment, alpha, gamma, epsilon, exp_updates):\n",
    "        self.env     = environment\n",
    "        self.alpha   = alpha\n",
    "        self.gamma   = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.exp_updates = exp_updates\n",
    "        \n",
    "    def reset(self):\n",
    "        '''Reset Seed, State-Action Value Function and History of Start-State Values'''\n",
    "        np.random.seed(0)\n",
    "        self.Q    = np.zeros((self.env.states, self.env.actions))\n",
    "        self.hist = np.zeros(self.exp_updates)\n",
    "        \n",
    "    def step(self, S, A):\n",
    "        '''Move according to Environment - get new State and Reward'''\n",
    "        B  = self.env.get_random_branch()\n",
    "        Sn = self.env.trans[S, A, B]\n",
    "        R  = self.env.get_reward()\n",
    "        return Sn, R\n",
    "    \n",
    "    def update_q(self, S, A, R, Sn):\n",
    "        '''Update State-Action Value Function with given transition'''\n",
    "        Am   = np.argmax(self.Q[Sn])\n",
    "        idx  = (S,  A)\n",
    "        idxn = (Sn, Am)\n",
    "        self.Q[idx] += self.alpha * (R * self.gamma * self.Q[idxn] - self.Q[idx])\n",
    "        \n",
    "    def epsilon_greedy(self, S):\n",
    "        '''Epsilon-Greedy Policy for selecting an Action'''\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            return np.argmax(self.Q[S])\n",
    "        else:\n",
    "            return np.random.randint(self.env.actions)\n",
    "        \n",
    "    def train_uniform(self):\n",
    "        '''Train uniformly by iterating over all states and their actions\n",
    "        and then updating them inplace'''\n",
    "        updates = 0\n",
    "        \n",
    "        for S in range(self.env.states):\n",
    "            for A in range(self.env.actions):\n",
    "                Sn, R = self.step(S, A)\n",
    "                \n",
    "                updates += 1\n",
    "                if updates < self.exp_updates:\n",
    "                    self.update_q(S, A, R, Sn)\n",
    "                    self.hist[updates] = self.get_start_state_value()\n",
    "                else:\n",
    "                    return self.hist        \n",
    "        \n",
    "    def train_trajectory_sample(self):\n",
    "        '''Train according to a Epsilon-Greedy Policy'''\n",
    "        updates = 0\n",
    "        \n",
    "        for episode in range(self.exp_updates):\n",
    "            S = self.env.get_start()\n",
    "\n",
    "            while not self.is_done():\n",
    "                A = self.epsilon_greedy(S)\n",
    "                Sn, R = self.step(S, A)\n",
    "\n",
    "                updates += 1\n",
    "                if updates < self.exp_updates:\n",
    "                    self.update_q(S, A, R, Sn)\n",
    "                    self.hist[updates] = self.get_start_state_value()\n",
    "                else:\n",
    "                    return self.hist\n",
    "                S = Sn\n",
    "                \n",
    "    def get_start_state_value(self):\n",
    "        '''Average over all transitions into the Starting State'''\n",
    "        start = self.env.get_start()\n",
    "        value = 0\n",
    "        \n",
    "        for state in range(self.env.states):\n",
    "            for action in range(self.env.actions):\n",
    "                for branch in range(self.env.branches):\n",
    "                    if self.env.trans[state, action, branch] == start:\n",
    "                        # '* 0.5' to average out the values \n",
    "                        # '1 / branches' is the probability of choosing the exact transition\n",
    "                        # 'Q[S, A]' is the expected value\n",
    "                        value += 0.5 * (1 / self.env.branches) * self.Q[state, action]\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def is_done(self):\n",
    "        '''True if the environment decides it's over'''\n",
    "        return self.env.is_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Parameters\n",
    "state_amount    = 10000\n",
    "actions         = 2\n",
    "branchin_factor = 1\n",
    "\n",
    "# Agent Parameters\n",
    "alpha           = 0.1\n",
    "gamma           = 1\n",
    "epsilon         = 0.1\n",
    "exp_updates     = 200000\n",
    "world           = Environment(state_amount,\n",
    "                              actions,\n",
    "                              branchin_factor)\n",
    "\n",
    "# Agent Definitions\n",
    "agent_uniform   = TabularPlanning(world, alpha, gamma, epsilon, exp_updates)\n",
    "agent_traject   = TabularPlanning(world, alpha, gamma, epsilon, exp_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_uniform.reset()\n",
    "uniform_reward = agent_uniform.train_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_traject.reset()\n",
    "traject_reward = agent_traject.train_trajectory_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(exp_updates)\n",
    "plt.plot(x, uniform_reward, label='uniform')\n",
    "plt.plot(x, traject_reward, label='trajectory')\n",
    "plt.xlabel('Expected Updates')\n",
    "plt.ylabel('Value of Start State')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
