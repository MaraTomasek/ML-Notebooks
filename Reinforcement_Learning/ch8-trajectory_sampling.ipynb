{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex8.8 Trajectory Sampling Exercise\n",
    "\n",
    "## 0. Problem Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* undiscounted episodic tasks\n",
    "* 2 actions from each state, each resulting in one of $b$ states, all equally likely\n",
    "* different random selection of $b$ states for each state-action pair\n",
    "* for each transition theres a $0.1$ chance of entering a terminal state\n",
    "* expected reward for each transition sampled from $\\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, state_amount, actions, branching_factor):\n",
    "        self.states   = state_amount\n",
    "        self.actions  = actions\n",
    "        self.branches = branching_factor\n",
    "        self.t_chance = 0.1\n",
    "        \n",
    "        self.build_transition_graph()\n",
    "        \n",
    "    def build_transition_graph(self):\n",
    "        '''Transition graph includes next state and reward for getting there'''\n",
    "        np.random.seed(0)\n",
    "        self.trans = np.random.randint(self.states, size=(self.states, self.actions, self.branches, 2))\n",
    "        \n",
    "        for state in range(self.states):\n",
    "            for action in range(self.actions):\n",
    "                for branch in range(self.branches):\n",
    "                    self.trans[state, action, branch, 1] = np.random.normal()\n",
    "    \n",
    "    def get_start(self):\n",
    "        '''Return the starting position (arbitrary but constant)'''\n",
    "        return 0\n",
    "    \n",
    "    def get_random_branch(self):\n",
    "        '''Choose a random branch'''\n",
    "        return np.random.randint(self.branches)\n",
    "\n",
    "    def is_terminal(self):\n",
    "        '''Call to determine if move is terminal'''\n",
    "        return np.random.rand() < self.t_chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One Step Tabular Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularPlanning:\n",
    "    \n",
    "    def __init__(self, environment, alpha, gamma, epsilon, exp_updates):\n",
    "        self.env     = environment\n",
    "        self.alpha   = alpha\n",
    "        self.gamma   = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.exp_updates = exp_updates\n",
    "        \n",
    "    def reset(self):\n",
    "        '''Reset Seed, State-Action Value Function and History of Start-State Values'''\n",
    "        np.random.seed(0)\n",
    "        self.Q    = np.zeros((self.env.states, self.env.actions))\n",
    "        self.hist = np.zeros(self.exp_updates)\n",
    "        \n",
    "    def step(self, S, A):\n",
    "        '''Move according to Environment - get new State and Reward'''\n",
    "        B     = self.env.get_random_branch()\n",
    "        Sn, R = self.env.trans[S, A, B]\n",
    "        return Sn, R\n",
    "    \n",
    "    def update_q(self, S, A, R, Sn):\n",
    "        '''Update State-Action Value Function with given transition'''\n",
    "        Am   = np.argmax(self.Q[Sn])\n",
    "        idx  = (S,  A)\n",
    "        idxn = (Sn, Am)\n",
    "        self.Q[idx] += (1 - self.env.t_chance) * self.alpha * (R + self.gamma * self.Q[idxn] - self.Q[idx])\n",
    "        \n",
    "    def epsilon_greedy(self, S, epsilon=None):\n",
    "        '''Epsilon-Greedy Policy for selecting an Action'''\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "            \n",
    "        if np.random.rand() > epsilon:\n",
    "            return np.argmax(self.Q[S])\n",
    "        else:\n",
    "            return np.random.randint(self.env.actions)\n",
    "        \n",
    "    def train_uniform(self):\n",
    "        '''Train uniformly by iterating over all states and their actions\n",
    "        and then updating them inplace'''\n",
    "        updates = 0\n",
    "        \n",
    "        while True:\n",
    "            for S in range(self.env.states):\n",
    "                for A in range(self.env.actions):\n",
    "                    if updates < self.exp_updates:\n",
    "                        Sn, R = self.step(S, A)\n",
    "\n",
    "                        self.update_q(S, A, R, Sn)\n",
    "                        self.hist[updates] = self.get_start_state_value()\n",
    "                    else:\n",
    "                        return self.hist\n",
    "                    \n",
    "                    updates += 1\n",
    "        \n",
    "    def train_trajectory_sample(self):\n",
    "        '''Train according to a Epsilon-Greedy Policy'''\n",
    "        updates = 0\n",
    "        \n",
    "        while True:\n",
    "            S = self.env.get_start()\n",
    "\n",
    "            while not self.is_done():\n",
    "                if updates < self.exp_updates:\n",
    "                    A = self.epsilon_greedy(S)\n",
    "                    Sn, R = self.step(S, A)\n",
    "\n",
    "                    self.update_q(S, A, R, Sn)\n",
    "                    self.hist[updates] = self.get_start_state_value()\n",
    "                    S = Sn\n",
    "                else:\n",
    "                    return self.hist\n",
    "                \n",
    "                updates += 1\n",
    "                \n",
    "    def get_start_state_value(self):\n",
    "        '''Follow Greedy Policy on Q and sum up rewards'''\n",
    "        start   = self.env.get_start()\n",
    "        values  = []\n",
    "        updates = 0\n",
    "        run_lim = 200\n",
    "        \n",
    "        while True:\n",
    "            S = start\n",
    "            \n",
    "            while not self.is_done():\n",
    "                if updates < run_lim:\n",
    "                    A = self.epsilon_greedy(S, epsilon=0)\n",
    "                    Sn, R = self.step(S, A)\n",
    "                    values.append(R)\n",
    "                    S = Sn\n",
    "                else:\n",
    "                    return np.mean(values)\n",
    "                \n",
    "                updates += 1\n",
    "    \n",
    "    def is_done(self):\n",
    "        '''True if the environment decides it's over'''\n",
    "        return self.env.is_terminal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Parameters\n",
    "state_amount      = 1000\n",
    "actions           = 2\n",
    "branching_factor  = 1\n",
    "branching_factor2 = 3\n",
    "\n",
    "# Agent Parameters\n",
    "alpha             = 0.9\n",
    "gamma             = 1\n",
    "epsilon           = 0.1\n",
    "exp_updates       = 20000\n",
    "world             = Environment(state_amount, actions, branching_factor)\n",
    "world2            = Environment(state_amount, actions, branching_factor2)\n",
    "\n",
    "# Agent Definitions\n",
    "agent_uniform     = TabularPlanning(world,  alpha, gamma, epsilon, exp_updates)\n",
    "agent_traject     = TabularPlanning(world,  alpha, gamma, epsilon, exp_updates)\n",
    "agent_uniform2    = TabularPlanning(world2, alpha, gamma, epsilon, exp_updates)\n",
    "agent_traject2    = TabularPlanning(world2, alpha, gamma, epsilon, exp_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Branching Factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_uniform.reset()\n",
    "uniform_reward = agent_uniform.train_uniform()\n",
    "\n",
    "agent_traject.reset()\n",
    "traject_reward = agent_traject.train_trajectory_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Branching Factor 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_uniform2.reset()\n",
    "uniform_reward2 = agent_uniform2.train_uniform()\n",
    "\n",
    "agent_traject2.reset()\n",
    "traject_reward2 = agent_traject2.train_trajectory_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(reward_uniform, reward_on_policy, exp_updates):\n",
    "    x = np.arange(exp_updates)\n",
    "    plt.plot(x, reward_uniform,   label='uniform')\n",
    "    plt.plot(x, reward_on_policy, label='on-policy')\n",
    "    plt.xlabel('Expected Updates')\n",
    "    plt.ylabel('Value of Start State')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Branching Factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(uniform_reward, traject_reward, exp_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Branching Factor 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(uniform_reward2, traject_reward2, exp_updates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
